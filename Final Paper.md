# 神经机器翻译

姓名：黄婉玉
学号：222023320082092
班级：计算机科学与技术01班

**摘**  **要：**机器翻译是使用计算机系统将文本或语音从一种自然语言自动翻译为另一种语言的过程。神经网络法已成为当前机器翻译方法的主流，神经机器翻译的基本原理是利用神经网络模拟人脑的语言处理机制，模仿双语者的翻译行为，实现文本的自动理解和转换。其中，编码器-解码器架构是神经机器翻译最经典的结构。尽管神经机器翻译在翻译速度和规模上远超人工翻译，但其翻译文本的个性和创造性仍有待提升。因此，译后编辑必不可少。

**关键词：** 机器翻译，神经机器翻译，编码器-解码器，Transformer模型，译后编辑

## 一、   引言

​		机器翻译是使用计算机系统将文本或语音从一种自然语言自动翻译为另一种语言的过程，试图用计算机来模拟人的翻译能力，是人工智能和自然语言处理领域的重要研究方向之一。由于人工智能领域中深度学习和神经网络的成功, 神经网络法代表着当前机器翻译方法的主流，在一些相对限定的专业领域, 如科技文献、法律文书、工业专利、说明书等, 机器翻译已经得到推广, 产生了巨大的社会效益和经济效益。但人类目前的机器翻译离彻底实现全自动高质量机器翻译的理想仍然存在不小的距离，需要人工的介入和编辑, 对机器翻译的原始产出进行加工与修改。

## 二、   神经机器翻译基本原理

​		随着深度学习技术的不断进步，神经网络的出现极大地推动了自然语言处理（NLP）领域各个下游任务的技术革新。其中，基于深度学习的神经机器翻译逐渐崭露头角，并逐步超越了当时主流的统计机器翻译方法。神经机器翻译凭借其结构简单、能够高效捕捉句子中的长依赖关系等优势，在机器翻译领域迅速占据了主导地位，成为了下一代机器翻译的范式。

​		神经网络翻译模型的主要思路是利用神经网络来模拟人脑的语言处理机制，模仿双语者的翻译行为，从而实现英语文本的自动理解和转换。这一领域最原始且最经典的结构便是编码器-解码器框架。

​		编码器-解码器架构是由Cho^[1]^等人提出的一种序列到序列（Sequence-to-Sequence，Seq2Seq）模型。在这一模型中，编码器负责深入分析输入的英语文本，以获取其语义信息；而解码器则负责将这些语义信息转换并输出为目标语言的文本。编码器和解码器可以由多种神经网络结构来实现，如循环神经网络（RNN）、卷积神经网络（CNN）等。经过训练与推理后，该模型即可生成目标语言的句子。

​		然而，无论是基于RNN还是基于CNN的模型，在处理长序列文本时，由于序列长度过长，都可能导致信息的丢失，从而影响模型的翻译性能。为了克服这一难题，2017年，Vaswani等人^[2]^提出了完全基于自注意力机制（Self-Attention）的Transformer模型。这一模型不依赖于任何循环单元或卷积单元，而是结合了RNN和CNN模型的优点。同时，Transformer模型支持并行化训练的特点，使得神经机器翻译（NMT）模型能够更快地收敛。

​		在Transformer模型结构中，最重要的部分便是多头自注意力（Multi-head Self-Attention）模块。这一模块也是改善模型对于长句子翻译质量的关键所在。多头自注意力机制的优势在于能够捕捉多种依赖关系，从而提高模型的翻译准确性。此外，由于Transformer模型能够在GPU上实现并行计算，因此模型训练的速度得到了大幅提升。在翻译质量上，Transformer模型也展现出了卓越的表现。正是由于Transformer模型的这些多方面优势，它在神经机器翻译领域表现优异，已经成为了最先进的神经网络模型结构之一。

## 三、   神经机器翻译的优势与挑战

​		神经机器翻译的优势在于，它能够高效地捕捉句子中的长依赖关系，这得益于其结构简单的特性。传统的统计机器翻译在处理长句子时往往力不从心，而神经机器翻译则凭借其先进的神经网络模型，如Transformer模型中的多头自注意力机制，能够轻松应对这一问题，显著提高翻译的准确性。此外，神经机器翻译模型在GPU上能够实现并行计算，使得模型训练的速度得到了大幅提升，进一步提升了其应用效率。

​		在翻译质量上，神经机器翻译也展现出了卓越的表现。它采用连续空间表示方法，无需进行词对齐、翻译规则抽取等繁琐步骤，便可以直接实现源语言到目标语言的映射。这种端到端的翻译模式不仅简化了翻译流程，还提高了翻译的流畅度和自然度。同时，神经机器翻译还具有较强的泛化能力，能够适应多种语言对之间的翻译任务，展现出巨大的应用潜力。

​		然而，神经机器翻译也存在缺陷。一方面，它受到词典大小和句子长度的限制。为了加快训练速度，神经机器翻译通常会将双语词典和句子长度限制在一定范围之内，这导致在处理一些包含未登录词或长句子的文本时，可能会出现翻译不准确或无法翻译的情况。另一方面，神经机器翻译在利用外部先验知识方面存在困难。它只采用双语训练数据，不要求额外先验知识，这在一定程度上限制了其翻译能力的提升。为了克服这些挑战，研究者们正在不断探索和优化神经机器翻译的模型架构和训练方法。

## 四、   译后编辑的重要性

​		目前, 参与神经机器翻译的单位越来越多。除了谷歌公司和Facebook公司之外, 中国的百度、搜狗、科大讯飞、阿里巴巴、腾讯、网易等公司也开展了神经机器翻译的研制, 整体水平都比较高, 在日常对话和新闻文本的机器翻译方面取得了良好的结果。从整体来看，机器翻译虽取得了巨大的成果，其翻译速度和一次性翻译的文本规模远非人工翻译所企及，但其翻译文本所呈现的个性和创造性远远不及人工翻译，不能译出源语文本的人际意义和语用内涵，尚未达到外宣翻译标准的水平。为了实现译文质量和翻译效率之间的平衡, 发挥人机交互的优势, 译后编辑成为翻译服务行业积极采用的翻译实施方式。

​		机器翻译的译后编辑 (MTPE) 是通过人工和部分自动化方式增强机器翻译的输出, 以满足特定质量目标的过程^[3]^，它已发展成为全球语言服务行业的新兴职业，例如谷歌、微软、赛门铁克、欧特克、莱博智、思迪、文思海辉等公司的部分产品或项目的本地化翻译都进行了机器翻译的译后编辑, 译后编辑的反馈可以帮助训练(统计)机译系统,受训的机译系统可望与译后编辑实践形成良性循环。

参考文献：

[1]   Cho K, Van Merriënboer B, Gulcehre C, et al. Learning phrase representations using RNN encoder-decoder for statistical machine translation[C]. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. 2014: 1724-1734.

[2]   Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances In Neural Information Processing Systems, 2017, 30.

[3]   [论机器翻译的译后编辑](https://kns.cnki.net/kcms2/article/abstract?v=OJyTKzW6Fer7VrE44pvRVZiaHfDE8w3PqdV-nKAcr2L2aA6kDrMtClFDjOXOL-OZhbBQTau_Yej6dMH5hOWqbcQbqroyuHpGmoodPbLORa7hd4DgD4XVNioja8SedbFTY3MV6ZGs6VmODp9aWbzXqA==&uniplatform=NZKPT&language=CHS). 崔启亮.中国翻译,2014(06)